## 设计要点总结

### 核心设计原则
- **分阶段递进**：先 Make it Work，再 Make it Scale
- **模型选型**：读扩散适合群聊，写扩散适合单聊
- **离线漫游**：用 `seq` 做断点续传，是现代 IM 的标配

### 关键改进点

| 优先级 | 改进项 | 阶段 | 说明 |
|--------|--------|------|------|
| 🔴 高 | 增加 `msg_id` 幂等字段 | 阶段一 | 客户端重试发送时防止消息重复 |
| 🔴 高 | 增加 ACK 确认机制 | 阶段一 | 服务端记录 `last_ack_seq` 精准计算未同步消息 |
| 🟡 中 | 心跳超时逻辑明确 | 阶段一 | 间隔 30s，连续 2-3 次无心跳视为离线 |
| 🟡 中 | 预留群成员表 | 阶段一 | 为后续群聊逻辑做准备 |
| 🟢 低 | Redis 过期策略 | 阶段二 | 防止 Inbox 无限增长 |
| 🟢 低 | Kafka 分区策略 | 阶段三 | 按 `conversation_id` Hash 保证消息顺序 |

---

### 第一阶段：单机 MVP (最小可行性产品)

**目标**：跑通基于 MySQL 的“读扩散”模型。实现群聊、发消息、**离线消息拉取**。
**耗时**：预计 5-7 天。

#### Step 1: 工程搭建与数据库设计 (Schema Design)

不要急着写 Go，先设计“图纸”, 把数据结构定义好。

1.  **建立 Go 项目结构** (遵循 `golang-standards`):

    ```text
    /cmd/server/main.go      // 入口
    /internal/service        // 业务逻辑
    /internal/repository     // 数据库操作 (DAO)
    /internal/model          // 结构体定义
    /pkg/protocol            // 协议定义
    /configs                 // 配置文件
    ```

2.  **MySQL 建表 (核心任务)**:
    需要三张表。不用搞太复杂，先跑通逻辑。

    ```sql
    -- 1. 消息表 (Timeline 的载体)
    -- 这里用自增ID模拟 Sequence ID，真实场景通常由 Redis 生成
    CREATE TABLE `timeline_message` (
        `id` BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
        `msg_id` VARCHAR(64) NOT NULL UNIQUE,   -- 客户端生成的唯一ID，用于幂等去重
        `conversation_id` VARCHAR(64) NOT NULL, -- 群ID，比如 "group_101"
        `sender_id` VARCHAR(64) NOT NULL,       -- 发送者ID
        `content` TEXT,                         -- 消息内容
        `msg_type` TINYINT DEFAULT 1,           -- 1:文本, 2:图片
        `status` TINYINT DEFAULT 0,             -- 0:发送中, 1:已送达, 2:已读
        `send_time` BIGINT NOT NULL,            -- 发送时间戳
        `created_at` TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        INDEX `idx_conv_id` (`conversation_id`),-- 核心索引：用于拉取 Timeline
        INDEX `idx_msg_id` (`msg_id`)           -- 幂等查询索引
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

    -- 2. 用户表
    CREATE TABLE `user` (
        `id` BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
        `user_id` VARCHAR(64) NOT NULL UNIQUE,
        `nickname` VARCHAR(64),
        `last_ack_seq` BIGINT DEFAULT 0,        -- 用户最后确认的消息序号
        `created_at` TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

    -- 3. 群成员表 (预留，第一阶段可先写死 group_1)
    CREATE TABLE `group_member` (
        `group_id` VARCHAR(64) NOT NULL,
        `user_id` VARCHAR(64) NOT NULL,
        `join_time` BIGINT NOT NULL,
        PRIMARY KEY (`group_id`, `user_id`)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
    ```

#### Step 2: 协议定义与 WebSocket 握手

利用 WebSocket 建立长连接。

1.  **定义通信协议 (`internal/model/protocol.go`)**:
    不要只发字符串，要定义标准的 JSON 结构。
    ```go
    // 客户端发给服务器的包
    type CmdType int
    const (
        CmdHeartbeat CmdType = 0 // 心跳
        CmdLogin     CmdType = 1 // 登录
        CmdChat      CmdType = 2 // 发送消息
        CmdPull      CmdType = 3 // 核心：主动拉取消息
        CmdAck       CmdType = 4 // 消息确认 (新增)
    )

    type InputPacket struct {
        Cmd     CmdType         `json:"cmd"`
        MsgId   string          `json:"msg_id,omitempty"`  // 客户端生成的消息唯一ID (幂等)
        Seq     int64           `json:"seq,omitempty"`     // 客户端当前的同步位点
        Payload json.RawMessage `json:"payload,omitempty"` // 具体数据
    }

    // 服务端发给客户端的包
    type OutputPacket struct {
        Cmd     CmdType     `json:"cmd"`
        Code    int         `json:"code"`              // 0:成功, 非0:失败
        MsgId   string      `json:"msg_id,omitempty"`  // 对应请求的消息ID
        Seq     int64       `json:"seq,omitempty"`     // 服务端分配的序列号
        Payload interface{} `json:"payload,omitempty"`
    }
    ```

2.  **心跳机制设计**:
      * **心跳间隔**：客户端每 30 秒发送一次 `CmdHeartbeat`
      * **超时判断**：服务端连续 2-3 次（60-90s）未收到心跳，判定为离线
      * **方式**：客户端主动发送，服务端被动接收并更新在线状态

3.  **实现 WebSocket 接入 (`cmd/server/main.go`)**:
      * 使用 `github.com/gorilla/websocket`。
      * 写一个 `ConnectionManager` (连接管理器)，用一个 `map[string]*websocket.Conn` 存所有在线连接。
      * **难点**：Go 的 Map 不是并发安全的，记得用 `sync.RWMutex` 保护这个 Map。

#### Step 3: 实现“先存储” (Persistence)

这是 Timeline 模型的第一步。

1.  **接收消息逻辑**:
      * 解析 WebSocket 传来的 `CmdChat` 包。
      * 提取 `conversation_id` 和 `content`。
2.  **写入 MySQL**:
      * 使用 GORM 或 sqlx 写入 `timeline_message` 表。
      * **注意**：这里依赖 MySQL 的 `AUTO_INCREMENT` 生成消息 ID (SeqID)。

#### Step 4: 实现“后同步” —— 在线推送 (Push)

消息存进去了，现在要推给在线的人。

1.  **简单的广播逻辑**:
      * 假设大家都在 "group\_1" (为了简化)。
      * 遍历 `ConnectionManager` 里的 Map。
      * 把刚才存入 MySQL 的消息（带上生成的 ID），JSON 序列化后 `conn.WriteJSON()` 给所有人。

#### Step 5: 实现“后同步” —— 离线拉取 (Pull) **(这是亮点)**

这是区分“玩具聊天室”和“现代 IM”的关键。

1.  **场景**: 用户断网了 10 分钟，重连。
2.  **客户端逻辑 (模拟)**: 发送 `CmdPull`，带上 `local_seq = 100` (假设他上次只看到了第 100 条)。
3.  **服务端逻辑**:
      * 收到 `CmdPull`。
      * 执行 SQL: `SELECT * FROM timeline_message WHERE conversation_id = ? AND id > 100 ORDER BY id ASC LIMIT 50`。
      * 把查出来的历史消息列表返给客户端。

> **✅ 第一阶段验收标准**:
> 可以用两个浏览器窗口（或 Postman），A 发消息，B 断开连接再重连，B 能自动收到刚才 A 发的消息。

-----

### 第二阶段：引入 Redis 优化 (信箱模型)

**目标**：解决 MySQL 压力，实现 1v1 聊天的**写扩散**优化。
**耗时**：3-5 天。

#### Step 1: 全局序列号生成器 (Sequence ID)

MySQL 的自增 ID 在分库分表下会失效，且性能差。我们改用 Redis。

1.  **部署 Redis**: Docker 跑起来。
2.  **改造 ID 生成逻辑**:
      * 每当有新消息发往 `group_A`。
      * Go 代码调用 Redis: `INCR im:seq:group_A`。
      * 拿到返回的数字（比如 1001），把它作为 `seq_id` 写入 MySQL，不再依赖 MySQL 自增。

#### Step 2: 实现单聊的"信箱" (Inbox)

对于 1v1 聊天，我们不再查大表，而是读小队列。

1.  **数据结构**: 使用 Redis Sorted Set (`ZADD` / `ZRANGEBYSCORE`)，比 List 更灵活。
2.  **Key 设计**: `im:inbox:{user_id}`。
3.  **发送逻辑改造**:
      * A 给 B 发消息。
      * 1.  存 MySQL 全量表 (归档用)。
      * 2.  **同时** 用 `ZADD im:inbox:B <seq_id> <msg_json>` 写入 Redis。
4.  **接收逻辑改造**:
      * B 在线时，直接推。
      * B 离线重连时，直接从 Redis `ZRANGEBYSCORE im:inbox:B <last_seq> +inf` 取数据，速度极快。
5.  **⚠️ 过期策略 (重要)**:
      * 方案一：设置 Key TTL，`EXPIRE im:inbox:B 604800` (7天)
      * 方案二：定期任务清理，`ZREMRANGEBYSCORE im:inbox:B -inf <threshold_seq>`
      * 方案三：限制条数，`ZREMRANGEBYRANK im:inbox:B 0 -1001` (只保留最新1000条)

> **✅ 第二阶段验收标准**:
> 查看 Redis Desktop Manager，发消息时能看到 Redis 里对应的 Key 在自增，Sorted Set 里有数据。

-----

### 第三阶段：架构解耦 (引入 Kafka)

**目标**：削峰填谷，防止数据库被瞬间流量打死。
**耗时**：3-4 天。

#### Step 1: 生产者改造 (Gateway)

1.  **部署 Kafka**: Docker 跑起来。
2.  **⚠️ 分区策略 (重要)**:
      * 按 `conversation_id` Hash 到固定分区：`partition = hash(conversation_id) % partition_count`
      * **原因**：保证同一会话的消息被同一个 Consumer 顺序消费，避免乱序
3.  **修改发送接口**:
      * 收到 WebSocket 消息后，**不再**直接写 MySQL/Redis。
      * 而是封装成一个 Event (Protobuf 或 JSON)，发送到 Kafka Topic `im_msg_inbox`。
      * **立即返回** 客户端 "已发送" (Ack)。

#### Step 2: 消费者开发 (Worker)

1.  **新建一个 Go 程序 (或者在同一个程序里开 Goroutine)**:
      * 监听 Kafka `im_msg_inbox`。
2.  **搬运逻辑**:
      * 收到消息 -\> 获取 Redis Seq ID -\> 写 MySQL -\> 写 Redis Inbox -\> 触发 WebSocket 推送。

#### Step 3: 优雅的推送 (可选)

  * 如果 Consumer 和 Gateway 是分离的（微服务），Consumer 处理完消息后，需要通知 Gateway 推送。
  * **简单做法**：Consumer 处理完，把消息再发到 Redis Pub/Sub，Gateway 订阅 Redis，收到通知后推给用户。

> **✅ 第三阶段验收标准**:
> 只有这一步做完，架构图才真正对应上了阿里那篇文章的图。Gateway 负责接入，Kafka 负责缓冲，Worker 负责落库。
